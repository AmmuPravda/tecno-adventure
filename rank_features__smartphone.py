# -*- coding: utf-8 -*-
"""Rank_Features _Smartphone.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p8nQTunvdWsa6K0-iJ563XRWPsJpXhod
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import warnings
warnings.filterwarnings('ignore')
from google.colab import drive
drive.mount('/content/ddrive')

dataset=pd.read_csv('MobileTrain.csv')

print(dataset.head())

dataset.describe(include='all')

dataset.info()

dataset.isnull().sum()

#Count Plot for Various Columns
for i in dataset:
  if(dataset[i].nunique()<30):
    sns.countplot(x = dataset[i])
    plt.show()

#Distplot for various columns
plt.figure(figsize=(30,10))
plt.subplot(331)
sns.distplot(dataset['battery_power'])
plt.subplot(332)
sns.distplot(dataset['clock_speed'])
plt.subplot(333)
sns.distplot(dataset['int_memory'])
plt.subplot(334)
sns.distplot(dataset['m_dep'])
plt.subplot(335)
sns.distplot(dataset['mobile_wt'])
plt.subplot(336)
sns.distplot(dataset['px_height'])
plt.subplot(337)
sns.distplot(dataset['px_width'])
plt.subplot(338)
sns.distplot(dataset['ram'])
plt.subplot(339)
sns.distplot(dataset['talk_time'])
plt.show()

dataset['isBluetooth']=''
for i in range(len(dataset)):
  if dataset['blue'][i]==0:
    dataset['isBluetooth'][i]='No'
  else:
      dataset['isBluetooth'][i]='Yes'
px.pie(data_frame=dataset,names='isBluetooth',title='Percentage of devices having bluetooth',hole=0.2)

dataset['isDualSim']=' '
for i in range(len(dataset)):
  if dataset['dual_sim'][i]==0:
    dataset['isDualSim'][i]='No'
  else:
      dataset['isDualSim'][i]='Yes'
px.pie(data_frame = dataset, names='isDualSim', title='Percentage of devices having dual sim', hole=0.2)

dataset['isFour_g']=' '
for i in range(len(dataset)):
  if dataset['four_g'][i]==0:
    dataset['isFour_g'][i]='No'
  else:
      dataset['isFour_g'][i]='Yes'
px.pie(data_frame = dataset, names='isFour_g', title='Percentage of devices having four g', hole=0.2)

dataset['isThree_g']=' '
for i in range(len(dataset)):
  if dataset['three_g'][i]==0:
    dataset['isThree_g'][i]='No'
  else:
      dataset['isFour_g'][i]='Yes'
px.pie(data_frame = dataset, names='isFour_g', title='Percentage of devices having three g', hole=0.2)

dataset['isTouchScreen']=' '
for i in range(len(dataset)):
  if dataset['touch_screen'][i]==0:
    dataset['isTouchScreen'][i]='No'
  else:
      dataset['isTouchScreen'][i]='Yes'
px.pie(data_frame = dataset, names='isTouchScreen', title='Percentage of devices having touch screen', hole=0.2)

dataset['isWifi']=' '
for i in range(len(dataset)):
  if dataset['wifi'][i]==0:
    dataset['isWifi'][i]='No'
  else:
      dataset['isWifi'][i]='Yes'
px.pie(data_frame = dataset, names='isWifi', title='Percentage of devices having Wifi', hole=0.2)

dataset['Cores']= ' '
for i in range(len(dataset)):
   if dataset['n_cores'][i] == 1:
       dataset['Cores'][i] = 'Single-Core'
   elif dataset['n_cores'][i] == 2:
        dataset['Cores'][i] = 'Dual-Core'
   elif dataset['n_cores'][i] == 3:
        dataset['Cores'][i] = 'Triple-Core'
   elif dataset['n_cores'][i] == 4:
        dataset['Cores'][i] = 'Quad-Core'
   elif dataset['n_cores'][i] == 5:
        dataset['Cores'][i] = 'Penta-Core'
   elif dataset['n_cores'][i] == 6:
        dataset['Cores'][i] = 'Hexa-Core'
   elif dataset['n_cores'][i] == 7:
        dataset['Cores'][i] = 'Hepta-Core'
   else:
        dataset['Cores'][i] = 'Octa-Core'
px.pie(data_frame = dataset, names= 'Cores', title = 'Percentage of devices having different types of Cores', hole= 0.2)

"""**Histograms**"""

px.histogram(data_frame = dataset, x ='isFour_g' , color = 'price_range' , title='comparison of devices sold having 4G or not')

px.histogram(data_frame = dataset, x ='isBluetooth' , color = 'price_range' , title='comparison of devices sold having Bluetooth or not')

px.histogram(data_frame = dataset, x ='isDualSim' , color = 'price_range' , title='comparison of devices sold having DualSim or not')

px.histogram(data_frame = dataset, x ='isTouchScreen' , color = 'price_range' , title='comparison of devices sold having TouchScreen or not')

px.histogram(data_frame = dataset, x ='isWifi' , color = 'price_range' , title='comparison of devices sold having Wifi or not')

print(dataset.shape)

print(dataset.head())

print(dataset.columns)

dataset.drop(['isBluetooth','isDualSim','isFour_g', 'isThree_g','isTouchScreen','isWifi','Cores'], axis = 1,inplace = True)

print(dataset.shape)

print(dataset.columns)

"""**Standard Scaler**"""

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
print(X.shape)

X = sc_X.fit_transform(X)
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X = sc_X.fit_transform(X)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state = 0)
sns.distplot(X)
sns.distplot(y)

"""**Logistic Regression**"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
log = LogisticRegression()
log.fit(X_train, y_train)
print("Training score of Logistic Regression is:{}".format(log.score(X_train,y_train) *100))
y_predlog = log.predict(X_test)
#print(y_predlog)
am_log = accuracy_score(y_test, y_predlog) *100
print("Accuracy of Logistic Regression classifier is:{}%".format(am_log))
print("Confusion Matrix of Logistic Regression classifier is: \n{}".format(confusion_matrix(y_test, y_predlog)))
print("{}".format(classification_report(y_test, y_predlog)))

""" **SVN  Classifier**"""

from sklearn.svm import SVC
svc = SVC(kernel = 'rbf')
svc.fit(X_train, y_train)
print("Training score of Suport Vector Machine classifier is:{}".format(svc.score(X_train,y_train)*100))
y_predsvm = svc.predict(X_test)

am_svm = accuracy_score(y_test, y_predsvm) * 100
print("Accuracy of Gaussian SVN Classifier is:{}%".format(am_svm))
print("Confusion Matrix of Gaussian Naive Bayes classifier is: \n{}".format(confusion_matrix(y_test,y_predsvm)))
print("{}".format(classification_report(y_test, y_predsvm)))

"""**Gaussian Naive Bayes  Classifier**"""

from sklearn.naive_bayes import GaussianNB
nb = GaussianNB()
nb.fit(X_train, y_train)
print("Training score of Gaussian Naive Bayes classifier is: {}".format(nb.score(X_train,y_train)*100))
y_prednb = nb.predict(X_test)
am_nb = accuracy_score(y_test, y_prednb) *100
print("Accuracy of Gaussian Naive Bayes classifier is:{}%".format(am_nb))
print("Confusion Matrix of Gaussian Naive Bayes classifier is: \n{}".format(confusion_matrix(y_test,y_prednb)))
print("{}".format(classification_report(y_test, y_prednb)))

"""**Random Forest Classifier**"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators = 300)
rf.fit(X_train, y_train)
print("Training score of Random Forest Classifier is:{}".format(rf.score(X_train,y_train)*100))
y_predrf = rf.predict(X_test)
am_rf = accuracy_score(y_test, y_predrf) *100
print("Accuracy of Random Forest Classifier is:{}%".format(am_rf))
print("Confusion Matrix of Random Forest classifier is: \n{}".format(confusion_matrix(y_test,y_predrf)))
print("{}".format(classification_report(y_test, y_predrf)))

"""**Decision Tree Classifier**"""

from sklearn.tree import DecisionTreeClassifier 
dt = DecisionTreeClassifier(criterion = 'entropy')
dt.fit(X_train, y_train)
print("Training score of DecisionTreeClassifier is:{}".format(dt.score(X_train,y_train)*100))
y_preddt = dt.predict(X_test)
am_dt = accuracy_score(y_test, y_preddt) *100
print("Accuracy of Decision Tree Classifier  is:{}%".format(am_dt))
print("Confusion Matrix of Decision Tree Classifier  is: \n{}".format(confusion_matrix(y_test,y_preddt)))
print("{}".format(classification_report(y_test, y_preddt)))

"""**Comparison of Classifiers**"""

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
classifiers = ['Logistic Regression','Naive bayes' , 'SVM', 'Decision Tree' ,'Random Forest']
lst_acc = [am_log, am_nb, am_svm, am_dt, am_rf]
df = pd.DataFrame({'Model' : classifiers, 'Accuracy' : lst_acc})
px.histogram(data_frame = df, x = 'Model' , y= 'Accuracy')

import numpy as np 
import matplotlib as mpl 
import matplotlib.pyplot as plt 
import seaborn as sns
## agg backend is used to create plot as a .png file
mpl.use('agg')

Result = pd.DataFrame(columns = ['Model','Training Score',"Accuracy Score"])
Result = Result.append({'Model' :'Logistic Regression','Training Score' :y_predlog,"Accuracy Score":round(am_log,2)},ignore_index = True)
Result = Result.append({'Model' :'Naive bayes','Training Score' :y_prednb,"Accuracy Score":round(am_nb,2)},ignore_index = True)
Result = Result.append({'Model' :'SVM','Training Score' :y_predsvm,"Accuracy Score":round(am_svm,2)},ignore_index = True)
Result = Result.append({'Model' :'Decision Tree','Training Score' :y_preddt,"Accuracy Score":round(am_dt,2)},ignore_index = True)
Result = Result.append({'Model' :'Random Forest','Training Score' :y_predrf,"Accuracy Score":round(am_rf,2)},ignore_index = True)
np.random.seed(10)
collectn_1 = np.random.normal(100, 10, 200)
collectn_2 = np.random.normal(80, 30, 200)
collectn_3 = np.random.normal(90, 20, 200)
collectn_4 = np.random.normal(70, 25, 200)

## combine these different collections into a list    
data_to_plot = [collectn_1, collectn_2, collectn_3, collectn_4]
# Create a figure instance
fig = plt.figure(1, figsize=(9, 6))

# Create an axes instance
ax = fig.add_subplot(111)

# Create the boxplot
bp = ax.boxplot(data_to_plot)

# Save the figure
fig.savefig('fig1.png', bbox_inches='tight')

## add patch_artist=True option to ax.boxplot() 
## to get fill color
bp = ax.boxplot(data_to_plot, patch_artist=True)

## change outline color, fill color and linewidth of the boxes
for box in bp['boxes']:
    # change outline color
    box.set( color='#7570b3', linewidth=2)
    # change fill color
    box.set( facecolor = '#1b9e77' )

## change color and linewidth of the whiskers
for whisker in bp['whiskers']:
    whisker.set(color='#7570b3', linewidth=2)

## change color and linewidth of the caps
for cap in bp['caps']:
    cap.set(color='#7570b3', linewidth=2)

## change color and linewidth of the medians
for median in bp['medians']:
    median.set(color='#b2df8a', linewidth=2)

## change the style of fliers and their fill
for flier in bp['fliers']:
    flier.set(marker='o', color='#e7298a', alpha=0.5)
    ## Custom x-axis labels
ax.set_xticklabels(['Logistic Regression', 'Naive bayes', 'SVM', 'Decision Tree4','Random Forest'])

## Remove top axes and right axes ticks
ax.get_xaxis().tick_bottom()
ax.get_yaxis().tick_left()

"""**Feature Selection**

**1.Univariate Selection**
"""

from pandas.core.internals.construction import dataclasses_to_dicts
import pandas as pd
import numpy as np
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
dataset=pd.read_csv('MobileTrain.csv')
X = dataset.iloc[:,0:20]  #independent columns
y = dataset.iloc[:,-1]    #target column i.e price range
#apply SelectKBest class to extract top 10 best features
bestfeatures = SelectKBest(score_func=chi2, k=10)
fit = bestfeatures.fit(X,y)
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(X.columns)
#concat two dataframes for better visualization 
featureScores = pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns = ['Specs','Score']  #naming the dataframe columns
print(featureScores.nlargest(10,'Score'))  #print 10 best features

"""**2. Feature Importance**"""

import pandas as pd
import numpy as np
dataset=pd.read_csv('MobileTrain.csv')
X = dataset.iloc[:,0:20]  #independent columns
y = dataset.iloc[:,-1]    #target column i.e price range
from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt
model = ExtraTreesClassifier()
model.fit(X,y)
print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers
#plot graph of feature importances for better visualization
feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
plt.show()

"""**3.Correlation Matrix with Heatmap**"""

import pandas as pd
import numpy as np
import seaborn as sns
dataset=pd.read_csv('MobileTrain.csv')
X = dataset.iloc[:,0:20]  #independent columns
y = dataset.iloc[:,-1]    #target column i.e price range
#get correlations of each features in dataset
corrmat = dataset.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(20,20))
#plot heat map
g=sns.heatmap(dataset[top_corr_features].corr(),annot=True,cmap="RdYlGn")

"""**Ranking**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
orders=pd.read_csv('MobileTrain.csv')

orders.head()

orders.rank()

orders.sort_values('ram',ascending=False).head

orders['Rank_by_price_range']=orders['price_range'].rank(ascending=False)

orders.head()

orders['Rank_by_price_range'] = orders['price_range'].rank(method ='average')

orders.head()

orders['Rank_by_price_range'] = orders['price_range'].rank(method ='min')

orders.head()

orders['Rank_by_price_range'] = orders['price_range'].rank(method ='min')

orders.head()

"""**Other Learning and Checking**

1.**one-dimensional ranking of features(Rank 1D)**
"""

from yellowbrick.datasets import load_credit
from yellowbrick.features import Rank1D

# Load the dataset
X = dataset.iloc[:,0:20]  #independent columns
y = dataset.iloc[:,-1]    #target column i.e price range

# Instantiate the 1D visualizer with the Sharpiro ranking algorithm
visualizer = Rank1D(algorithm='shapiro')

visualizer.fit(X, y)           # Fit the data to the visualizer
visualizer.transform(X)        # Transform the data
visualizer.show()

"""**2.Two-dimensional ranking of features(Rank 2D)**"""

from yellowbrick.datasets import load_credit
from yellowbrick.features import Rank2D

# Load the  dataset
X = dataset.iloc[:,0:20]  #independent columns
y = dataset.iloc[:,-1] 

# Instantiate the visualizer with the Pearson ranking algorithm
visualizer = Rank2D(algorithm='pearson')

visualizer.fit(X, y)           # Fit the data to the visualizer
visualizer.transform(X)        # Transform the data
visualizer.show()

"""**3.quick methods**"""

from yellowbrick.datasets import load_concrete
from yellowbrick.features import rank1d, rank2d

# Load the concrete dataset
X= dataset.iloc[:,0:20] 

_, axes = plt.subplots(ncols=2, figsize=(20,14))

rank1d(X, ax=axes[0], show=False)
rank2d(X, ax=axes[1], show=False)
plt.show()









